{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphQL endpoint performance testing notebook\n",
    "\n",
    "This notebook will help you run some performance testing in your GraphQL artifact. You can use this notebooks to test a single GraphQL endpoint.\n",
    "\n",
    "What you will need:\n",
    "1. A GraphQL artifact attached to one or more data sources\n",
    "2. Filling out the values in the \"variables\" section below\n",
    "3. You are good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1: Fill out the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell defined some important variables for the test setup.\n",
    "TODO: replace the values here with the correct values for your testing!\n",
    "'''\n",
    "\n",
    "# Change this to your GraphQL endpoint!\n",
    "# To find the correct value, go to your GraphQL artifact in the Fabric portal, and, in the top menu\n",
    "# ribbon, you will find a \"Copy Endpoint\" button\n",
    "graphql_endpoint = '[ENDPOINT]'\n",
    "# Change this where you want your results file to live\n",
    "# You can also leave it as default - the code will create the results file for you if it does not exist.\n",
    "results_file_path = './results_test.csv'\n",
    "\n",
    "# Change this to the query you wish to run in your GraphQL artifact during the performance experiment!\n",
    "# You can also leave this value as default if this query would work in your GraphQL artifact\n",
    "query_gql = \"\"\"\n",
    "    query {\n",
    "} \n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "# Chang ethis to the variables you wish to run with your GraphQL query! \n",
    "# You can also leave this value as default (if it makes sense for your query) or use an empty dictionary \n",
    "# if you don't with to use variables.\n",
    "# eg:\n",
    "# variables = {\n",
    "# }\n",
    "\n",
    "variables = {\n",
    "    \"variable_name\": \"variable_value\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Run the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all libraries needed\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "import requests\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run this cell to run the performance test!\n",
    "\n",
    "As soon as you run this cell, you will be prompte to log in into your account. Make sure to log in with an account\n",
    "that has access to both the GRAPHQL artifact you wish to use AND the DATASOURCE(S) your GraphQL artifact has attached.\n",
    "\n",
    "The experiment will run for 1 hour and save all the results in the file defined in the variable \"results_file_path\"\n",
    "'''\n",
    "\n",
    "# Setting up the results file\n",
    "with open(results_file_path,'a+') as fd:\n",
    "    if os.stat(results_file_path).st_size == 0:\n",
    "        fd.write(\"Root Activity ID, Date, Total elapsed time (ms), Overhead (ms), Data Source Query Execution (ms)\" + \"\\n\")\n",
    "\n",
    "# GraphQL setup\n",
    "app = InteractiveBrowserCredential()\n",
    "scp = 'https://analysis.windows.net/powerbi/api/user_impersonation'\n",
    "result = app.get_token(scp)\n",
    "\n",
    "if not result.token:\n",
    "    print('Error:', \"Could not get access token\")\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {result.token}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "overhead_times = []\n",
    "db_query_execution_times = []\n",
    "total_times = []\n",
    "# Use session to re-use the connections instead of creating a new one for each request\n",
    "session = requests.Session()\n",
    "\n",
    "# Issue GraphQL request\n",
    "try:\n",
    "    while True:\n",
    "        # Running the request\n",
    "        start = time.perf_counter()\n",
    "        response = session.post(graphql_endpoint, json={'query': query_gql, 'variables': variables}, headers=headers)\n",
    "        end = time.perf_counter()\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Getting the data\n",
    "        data = response.json()\n",
    "        # -- RAID\n",
    "        root_activity_id = response.headers['x-ms-root-activity-id']\n",
    "        # -- Date\n",
    "        date = response.headers['Date'].split(\",\")[1]\n",
    "        # -- Performance\n",
    "        overhead = \"N/A\"\n",
    "        db_time = \"N/A\"\n",
    "        if('x-ms-latency' in response.headers.values()):\n",
    "            latency = response.headers['x-ms-latency']\n",
    "            overhead = latency.split(\";\")[0]\n",
    "            overhead_times.append(overhead)\n",
    "\n",
    "            db_time = latency.split(\";\")[1]\n",
    "            db_query_execution_times.append(db_time)\n",
    "\n",
    "        performance_total = (end - start) * 1000\n",
    "        total_times.append(performance_total)\n",
    "\n",
    "        # Saving data to file\n",
    "        with open(results_file_path,'a') as fd:\n",
    "            fd.write(root_activity_id + \",\" + date + \",\" + str(performance_total) + \",\" + overhead + \",\" + db_time + \"\\n\")\n",
    "        \n",
    "        # Waiting 2 seconds\n",
    "        time.sleep(2)\n",
    "    \n",
    "except Exception as error:\n",
    "    print(f\"Query failed with error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### See results from latest run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.DataFrame(data={\n",
    "    \"Average\": [np.mean(total_times)],\n",
    "    \"Standart Deviation\" : [np.std(total_times)],\n",
    "    \"P90\": [np.percentile(total_times, 90)],\n",
    "    \"P95\": [np.percentile(total_times, 95)],\n",
    "    \"P99\": [np.percentile(total_times, 99)]\n",
    "})\n",
    "total_df = total_df.style.set_caption(\"Total Time Statistics\")\n",
    "display(total_df)\n",
    "\n",
    "if len(overhead_times) > 0:\n",
    "    overhead_df = pd.DataFrame(data={\n",
    "        \"Average\": [np.mean(overhead_times)],\n",
    "        \"Standart Deviation\" : [np.std(overhead_times)],\n",
    "        \"P90\": [np.percentile(overhead_times, 90)],\n",
    "        \"P95\": [np.percentile(overhead_times, 95)],\n",
    "        \"P99\": [np.percentile(overhead_times, 99)]\n",
    "    })\n",
    "    overhead_df = overhead_df.style.set_caption(\"Overhead Time Statistics\")\n",
    "    display(overhead_df)\n",
    "\n",
    "if len(db_query_execution_times) > 0:\n",
    "    db_query_execution_df = pd.DataFrame(data={\n",
    "        \"Average\": [np.mean(db_query_execution_times)],\n",
    "        \"Standart Deviation\" : [np.std(db_query_execution_times)],\n",
    "        \"P90\": [np.percentile(db_query_execution_times, 90)],\n",
    "        \"P95\": [np.percentile(db_query_execution_times, 95)],\n",
    "        \"P99\": [np.percentile(db_query_execution_times, 99)]\n",
    "    })\n",
    "    db_query_execution_df = db_query_execution_df.style.set_caption(\"Datasource Query Execution Time Statistics\")\n",
    "    display(total_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
